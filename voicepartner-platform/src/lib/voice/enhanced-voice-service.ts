'use client'

export interface VoiceConfig {
  elevenLabsApiKey?: string
  openaiApiKey?: string
  language: string
  voiceId?: string
}

export interface ConversationMessage {
  role: 'user' | 'assistant' | 'system'
  content: string
  timestamp: Date
  audioUrl?: string
}

export class EnhancedVoiceService {
  private config: VoiceConfig
  private isRecording = false
  private mediaRecorder: MediaRecorder | null = null
  private audioChunks: Blob[] = []
  private conversationHistory: ConversationMessage[] = []

  constructor(config: VoiceConfig) {
    this.config = config
  }

  // Enhanced Text-to-Speech with ultra-low latency optimization
  async textToSpeech(text: string, voiceId?: string): Promise<ArrayBuffer> {
    const startTime = performance.now()
    
    // Ultra-fast demo mode for minimal latency
    if (!this.config.elevenLabsApiKey) {
      // Create optimized synthetic audio response with minimal delay
      const response = await this.createOptimizedSyntheticAudio(text)
      const endTime = performance.now()
      console.log(`TTS Response time: ${Math.round(endTime - startTime)}ms`)
      return response
    }

    try {
      // Optimized ElevenLabs API call with minimal settings for speed
      const response = await fetch(`https://api.elevenlabs.io/v1/text-to-speech/${voiceId || this.config.voiceId || 'ErXwobaYiN019PkySvjV'}`, {
        method: 'POST',
        headers: {
          'Accept': 'audio/mpeg',
          'Content-Type': 'application/json',
          'xi-api-key': this.config.elevenLabsApiKey
        },
        body: JSON.stringify({
          text,
          model_id: 'eleven_turbo_v2', // Fastest model
          voice_settings: {
            stability: 0.5, // Reduced for speed
            similarity_boost: 0.5, // Reduced for speed  
            style: 0.0, // Minimal style processing
            use_speaker_boost: false // Disabled for speed
          },
          optimize_streaming_latency: 4, // Maximum optimization
          output_format: 'mp3_22050_32' // Lower quality for speed
        })
      })

      if (!response.ok) {
        console.warn('ElevenLabs API failed, using synthetic fallback')
        return await this.createOptimizedSyntheticAudio(text)
      }

      const audioBuffer = await response.arrayBuffer()
      const endTime = performance.now()
      console.log(`TTS Response time: ${Math.round(endTime - startTime)}ms`)
      return audioBuffer
    } catch (error) {
      console.warn('TTS error, using synthetic fallback:', error)
      return await this.createOptimizedSyntheticAudio(text)
    }
  }

  // Create optimized synthetic audio for ultra-low latency demo
  private async createOptimizedSyntheticAudio(text: string): Promise<ArrayBuffer> {
    // Simulate ultra-fast response with minimal processing delay
    await new Promise(resolve => setTimeout(resolve, 50)) // 50ms simulated processing
    
    const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
    const sampleRate = 22050 // Lower sample rate for speed
    const duration = Math.max(0.3, text.length * 0.05) // Faster speech rate
    const buffer = audioContext.createBuffer(1, sampleRate * duration, sampleRate)
    
    // Generate synthetic beep pattern for demo
    const data = buffer.getChannelData(0)
    for (let i = 0; i < data.length; i++) {
      const time = i / sampleRate
      data[i] = Math.sin(2 * Math.PI * 440 * time) * Math.exp(-time * 3) * 0.3
    }
    
    // Convert to minimal ArrayBuffer format
    const arrayBuffer = new ArrayBuffer(data.length * 2)
    const view = new Int16Array(arrayBuffer)
    for (let i = 0; i < data.length; i++) {
      view[i] = Math.max(-32767, Math.min(32767, data[i] * 32767))
    }
    
    return arrayBuffer
  }

  // Enhanced playback with promise resolution
  async playAudio(audioBuffer: ArrayBuffer): Promise<void> {
    return new Promise((resolve, reject) => {
      try {
        const blob = new Blob([audioBuffer], { type: 'audio/mpeg' })
        const audioUrl = URL.createObjectURL(blob)
        const audio = new Audio(audioUrl)
        
        audio.onended = () => {
          URL.revokeObjectURL(audioUrl)
          resolve()
        }
        
        audio.onerror = (error) => {
          URL.revokeObjectURL(audioUrl)
          reject(new Error('Audio playback failed'))
        }
        
        // Set volume and play
        audio.volume = 0.8
        audio.play().catch(reject)
      } catch (error) {
        reject(error)
      }
    })
  }

  // Start recording with enhanced error handling
  async startRecording(): Promise<void> {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 44100
        } 
      })
      
      this.mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      })
      
      this.audioChunks = []

      this.mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          this.audioChunks.push(event.data)
        }
      }

      this.mediaRecorder.start(1000) // Collect data every second
      this.isRecording = true
    } catch (error) {
      console.error('Failed to start recording:', error)
      throw new Error('Mikrofon-Zugriff verweigert oder nicht verf√ºgbar')
    }
  }

  // Stop recording with proper cleanup
  async stopRecording(): Promise<Blob> {
    return new Promise((resolve, reject) => {
      if (!this.mediaRecorder || !this.isRecording) {
        reject(new Error('No active recording'))
        return
      }

      this.mediaRecorder.onstop = () => {
        const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm;codecs=opus' })
        this.isRecording = false
        
        // Stop all tracks to free resources
        if (this.mediaRecorder?.stream) {
          this.mediaRecorder.stream.getTracks().forEach(track => {
            track.stop()
          })
        }
        
        resolve(audioBlob)
      }

      this.mediaRecorder.onerror = (error) => {
        reject(error)
      }

      this.mediaRecorder.stop()
    })
  }

  // Ultra-low latency Speech-to-Text with aggressive optimization
  async speechToText(audioBlob: Blob): Promise<string> {
    const startTime = performance.now()
    
    // Primary: Optimized Web Speech API for minimal latency
    try {
      const result = await this.optimizedWebSpeechRecognition()
      const endTime = performance.now()
      console.log(`STT Response time: ${Math.round(endTime - startTime)}ms`)
      return result
    } catch (error) {
      console.warn('Optimized Web Speech API failed:', error)
    }

    // Fallback: OpenAI Whisper API (if available)
    if (this.config.openaiApiKey) {
      try {
        const formData = new FormData()
        formData.append('file', audioBlob, 'audio.webm')
        formData.append('model', 'whisper-1')
        formData.append('language', this.config.language === 'de' ? 'de' : 'en')
        formData.append('response_format', 'json')
        formData.append('temperature', '0') // Deterministic for speed

        const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${this.config.openaiApiKey}`
          },
          body: formData
        })

        if (response.ok) {
          const result = await response.json()
          const endTime = performance.now()
          console.log(`STT Response time: ${Math.round(endTime - startTime)}ms`)
          return result.text.trim()
        }
      } catch (error) {
        console.warn('Whisper API error:', error)
      }
    }

    // Ultra-fast fallback: Optimized demo simulation
    const result = this.simulateOptimizedTranscription()
    const endTime = performance.now()
    console.log(`STT Response time: ${Math.round(endTime - startTime)}ms`)
    return result
  }

  // Optimized Web Speech API with minimal latency settings
  private async optimizedWebSpeechRecognition(): Promise<string> {
    return new Promise((resolve, reject) => {
      if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
        reject(new Error('Speech recognition not supported in this browser'))
        return
      }

      const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition
      const recognition = new SpeechRecognition()
      
      // Optimized settings for minimal latency
      recognition.lang = this.config.language === 'de' ? 'de-DE' : 'en-US'
      recognition.continuous = false
      recognition.interimResults = true // Enable for faster feedback
      recognition.maxAlternatives = 1
      recognition.grammars = null // Disable grammar for speed

      let timeoutId: NodeJS.Timeout | null = null
      let finalResult = ''

      recognition.onstart = () => {
        // Reduced timeout for faster response
        timeoutId = setTimeout(() => {
          recognition.stop()
          if (finalResult) {
            resolve(finalResult)
          } else {
            reject(new Error('Speech recognition timeout'))
          }
        }, 3000) // 3 second timeout for speed
      }

      recognition.onresult = (event: any) => {
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i]
          if (result.isFinal) {
            finalResult = result[0].transcript.trim()
            if (timeoutId) clearTimeout(timeoutId)
            resolve(finalResult)
            return
          } else {
            // Use interim results for ultra-low latency
            const interimResult = result[0].transcript.trim()
            if (interimResult.length > 3) { // Minimum viable result
              if (timeoutId) clearTimeout(timeoutId)
              resolve(interimResult)
              return
            }
          }
        }
      }

      recognition.onerror = (event: any) => {
        if (timeoutId) clearTimeout(timeoutId)
        reject(new Error(`Speech recognition error: ${event.error}`))
      }

      recognition.onend = () => {
        if (timeoutId) clearTimeout(timeoutId)
        if (finalResult) {
          resolve(finalResult)
        }
      }

      try {
        recognition.start()
      } catch (error) {
        if (timeoutId) clearTimeout(timeoutId)
        reject(error)
      }
    })
  }

  // Legacy Web Speech API implementation (kept for compatibility)
  private async webSpeechRecognition(): Promise<string> {
    return this.optimizedWebSpeechRecognition()
  }

  // Optimized ultra-fast simulation for minimal latency demo
  private simulateOptimizedTranscription(): string {
    // Minimal processing delay for ultra-low latency
    const conversationStep = this.conversationHistory.length
    
    // Streamlined response patterns for speed
    const quickResponses = [
      'Hallo, ich m√∂chte einen Termin buchen',
      'Mein Name ist Max Mustermann', 
      'Meine Telefonnummer ist 030 12345678',
      'Ich h√§tte gerne eine Beratung',
      'Morgen um 14 Uhr w√§re gut',
      'Ja, das passt perfekt',
      'Vielen Dank f√ºr die Hilfe'
    ]

    const index = Math.min(conversationStep, quickResponses.length - 1)
    return quickResponses[index]
  }

  // Legacy simulation method (kept for compatibility)
  private simulateTranscription(): string {
    return this.simulateOptimizedTranscription()
  }

  // Add message to conversation history
  addToHistory(role: 'user' | 'assistant' | 'system', content: string, audioUrl?: string) {
    this.conversationHistory.push({
      role,
      content,
      timestamp: new Date(),
      audioUrl
    })
  }

  // Get conversation history
  getHistory(): ConversationMessage[] {
    return [...this.conversationHistory]
  }

  // Clear conversation history
  clearHistory() {
    this.conversationHistory = []
  }

  // Check recording status
  getIsRecording(): boolean {
    return this.isRecording
  }

  // Update configuration
  updateConfig(config: Partial<VoiceConfig>) {
    this.config = { ...this.config, ...config }
  }

  // Test TTS connection
  async testConnection(): Promise<boolean> {
    if (!this.config.elevenLabsApiKey) return false
    
    try {
      const response = await fetch('https://api.elevenlabs.io/v1/voices', {
        headers: {
          'xi-api-key': this.config.elevenLabsApiKey
        }
      })
      return response.ok
    } catch {
      return false
    }
  }
}